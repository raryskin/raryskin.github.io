<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Authors | LInC Lab</title>
    <link>/authors/</link>
      <atom:link href="/authors/index.xml" rel="self" type="application/rss+xml" />
    <description>Authors</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 05 Feb 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Authors</title>
      <link>/authors/</link>
    </image>
    
    <item>
      <title></title>
      <link>/authors/admin/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/authors/admin/</guid>
      <description>&lt;p&gt;Language endows us with the ability to communicate the contents of our mind with another person. But how do we decode another person&amp;rsquo;s thoughts and intentions from a sequence of sounds (speech) or shapes (letters, signs)? This challenge seems compounded by the fact that transmission of this sequence is often imperfect &amp;ndash; speakers and listeners make errors, don&amp;rsquo;t have the same prior knowledge, and sometimes even see different things. Yet, communication with another person typically feels effortless. In the Language, Interaction, &amp;amp; Cognition (LInC) Lab, we take a cognitive (neuro)science approach to understanding &lt;strong&gt;how the human language processing system extracts meaning from uncertain and variable input&lt;/strong&gt;. In particular, we investigate how &lt;strong&gt;learning and memory mechanisms&lt;/strong&gt; ground language understanding in the speaker and listener&amp;rsquo;s &lt;strong&gt;visuo-spatial and temporal contexts&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;To get more information about ongoing projects, please visit the &lt;a href=&#34;#research&#34;&gt;research&lt;/a&gt; page or check individual lab member&amp;rsquo;s pages for a full list of their publications. If you would like to participate in the lab&amp;rsquo;s research or are interested in joining the lab, get in touch via &lt;a href=&#34;#contact&#34;&gt;e-mail&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/authors/cindy_fang/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/authors/cindy_fang/</guid>
      <description>&lt;p&gt;Xinzhu (Cindy) Fang is a PhD student in the Department of Cognitive &amp;amp; Information Sciences. She is interested in the cognitive neuroscience of language and music processing.&lt;/p&gt;

&lt;p&gt;Check Xinzhu&amp;rsquo;s &lt;a href=&#34;https://xf15.github.io/&#34; target=&#34;_blank&#34;&gt;personal website&lt;/a&gt; for a CV and up-to-date list of publications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/authors/rachel_ryskin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/authors/rachel_ryskin/</guid>
      <description>&lt;p&gt;Rachel Ryskin is an Assistant Professor in the Department of Cognitive &amp;amp; Information Sciences. She wants to figure out what cognitive processes and computations enable humans to understand each other. In particular, she studies how learning and context-specific predictions help the human mind and brain deal with the noise and ambiguity present in everyday communication.&lt;/p&gt;

&lt;p&gt;Check her &lt;a href=&#34;https://web.mit.edu/ryskin/www/&#34; target=&#34;_blank&#34;&gt;personal website&lt;/a&gt; for an up-to-date list of publications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/authors/vera_nicolette_sanchez/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/authors/vera_nicolette_sanchez/</guid>
      <description>&lt;p&gt;Vera Nicolette Sanchez is a Cognitive Science major at UC Merced. She is interested in human-computer interaction, UX design, and artificial intelligence.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
